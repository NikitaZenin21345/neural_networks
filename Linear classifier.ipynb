{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikze\\AppData\\Local\\Temp\\ipykernel_13060\\3146107952.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(x*x), 2*x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.484015\n",
      "Epoch 1, loss: 2.356050\n",
      "Epoch 2, loss: 2.317979\n",
      "Epoch 3, loss: 2.306623\n",
      "Epoch 4, loss: 2.303297\n",
      "Epoch 5, loss: 2.302304\n",
      "Epoch 6, loss: 2.301991\n",
      "Epoch 7, loss: 2.301899\n",
      "Epoch 8, loss: 2.301881\n",
      "Epoch 9, loss: 2.301857\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x214579339e0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301890\n",
      "Epoch 1, loss: 2.301866\n",
      "Epoch 2, loss: 2.301834\n",
      "Epoch 3, loss: 2.301874\n",
      "Epoch 4, loss: 2.301892\n",
      "Epoch 5, loss: 2.301864\n",
      "Epoch 6, loss: 2.301880\n",
      "Epoch 7, loss: 2.301844\n",
      "Epoch 8, loss: 2.301878\n",
      "Epoch 9, loss: 2.301869\n",
      "Epoch 10, loss: 2.301858\n",
      "Epoch 11, loss: 2.301852\n",
      "Epoch 12, loss: 2.301887\n",
      "Epoch 13, loss: 2.301867\n",
      "Epoch 14, loss: 2.301868\n",
      "Epoch 15, loss: 2.301866\n",
      "Epoch 16, loss: 2.301873\n",
      "Epoch 17, loss: 2.301863\n",
      "Epoch 18, loss: 2.301867\n",
      "Epoch 19, loss: 2.301881\n",
      "Epoch 20, loss: 2.301849\n",
      "Epoch 21, loss: 2.301864\n",
      "Epoch 22, loss: 2.301849\n",
      "Epoch 23, loss: 2.301870\n",
      "Epoch 24, loss: 2.301869\n",
      "Epoch 25, loss: 2.301857\n",
      "Epoch 26, loss: 2.301877\n",
      "Epoch 27, loss: 2.301867\n",
      "Epoch 28, loss: 2.301849\n",
      "Epoch 29, loss: 2.301854\n",
      "Epoch 30, loss: 2.301862\n",
      "Epoch 31, loss: 2.301862\n",
      "Epoch 32, loss: 2.301853\n",
      "Epoch 33, loss: 2.301877\n",
      "Epoch 34, loss: 2.301866\n",
      "Epoch 35, loss: 2.301865\n",
      "Epoch 36, loss: 2.301858\n",
      "Epoch 37, loss: 2.301851\n",
      "Epoch 38, loss: 2.301872\n",
      "Epoch 39, loss: 2.301859\n",
      "Epoch 40, loss: 2.301866\n",
      "Epoch 41, loss: 2.301858\n",
      "Epoch 42, loss: 2.301863\n",
      "Epoch 43, loss: 2.301867\n",
      "Epoch 44, loss: 2.301864\n",
      "Epoch 45, loss: 2.301872\n",
      "Epoch 46, loss: 2.301868\n",
      "Epoch 47, loss: 2.301883\n",
      "Epoch 48, loss: 2.301870\n",
      "Epoch 49, loss: 2.301858\n",
      "Epoch 50, loss: 2.301856\n",
      "Epoch 51, loss: 2.301865\n",
      "Epoch 52, loss: 2.301870\n",
      "Epoch 53, loss: 2.301870\n",
      "Epoch 54, loss: 2.301863\n",
      "Epoch 55, loss: 2.301847\n",
      "Epoch 56, loss: 2.301873\n",
      "Epoch 57, loss: 2.301872\n",
      "Epoch 58, loss: 2.301868\n",
      "Epoch 59, loss: 2.301851\n",
      "Epoch 60, loss: 2.301860\n",
      "Epoch 61, loss: 2.301859\n",
      "Epoch 62, loss: 2.301867\n",
      "Epoch 63, loss: 2.301865\n",
      "Epoch 64, loss: 2.301871\n",
      "Epoch 65, loss: 2.301869\n",
      "Epoch 66, loss: 2.301848\n",
      "Epoch 67, loss: 2.301860\n",
      "Epoch 68, loss: 2.301864\n",
      "Epoch 69, loss: 2.301860\n",
      "Epoch 70, loss: 2.301864\n",
      "Epoch 71, loss: 2.301873\n",
      "Epoch 72, loss: 2.301851\n",
      "Epoch 73, loss: 2.301871\n",
      "Epoch 74, loss: 2.301889\n",
      "Epoch 75, loss: 2.301869\n",
      "Epoch 76, loss: 2.301877\n",
      "Epoch 77, loss: 2.301871\n",
      "Epoch 78, loss: 2.301864\n",
      "Epoch 79, loss: 2.301866\n",
      "Epoch 80, loss: 2.301871\n",
      "Epoch 81, loss: 2.301868\n",
      "Epoch 82, loss: 2.301862\n",
      "Epoch 83, loss: 2.301852\n",
      "Epoch 84, loss: 2.301876\n",
      "Epoch 85, loss: 2.301856\n",
      "Epoch 86, loss: 2.301868\n",
      "Epoch 87, loss: 2.301865\n",
      "Epoch 88, loss: 2.301863\n",
      "Epoch 89, loss: 2.301878\n",
      "Epoch 90, loss: 2.301864\n",
      "Epoch 91, loss: 2.301873\n",
      "Epoch 92, loss: 2.301861\n",
      "Epoch 93, loss: 2.301872\n",
      "Epoch 94, loss: 2.301884\n",
      "Epoch 95, loss: 2.301877\n",
      "Epoch 96, loss: 2.301870\n",
      "Epoch 97, loss: 2.301869\n",
      "Epoch 98, loss: 2.301898\n",
      "Epoch 99, loss: 2.301862\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.143206\n",
      "Epoch 1, loss: 2.143117\n",
      "Epoch 2, loss: 2.143056\n",
      "Epoch 3, loss: 2.142997\n",
      "Epoch 4, loss: 2.142937\n",
      "Epoch 5, loss: 2.142850\n",
      "Epoch 6, loss: 2.142782\n",
      "Epoch 7, loss: 2.142753\n",
      "Epoch 8, loss: 2.142685\n",
      "Epoch 9, loss: 2.142625\n",
      "Epoch 10, loss: 2.142555\n",
      "Epoch 11, loss: 2.142495\n",
      "Epoch 12, loss: 2.142450\n",
      "Epoch 13, loss: 2.142387\n",
      "Epoch 14, loss: 2.142305\n",
      "Epoch 15, loss: 2.142241\n",
      "Epoch 16, loss: 2.142193\n",
      "Epoch 17, loss: 2.142118\n",
      "Epoch 18, loss: 2.142072\n",
      "Epoch 19, loss: 2.142019\n",
      "Epoch 20, loss: 2.141936\n",
      "Epoch 21, loss: 2.141879\n",
      "Epoch 22, loss: 2.141820\n",
      "Epoch 23, loss: 2.141752\n",
      "Epoch 24, loss: 2.141701\n",
      "Epoch 25, loss: 2.141625\n",
      "Epoch 26, loss: 2.141560\n",
      "Epoch 27, loss: 2.141516\n",
      "Epoch 28, loss: 2.141487\n",
      "Epoch 29, loss: 2.141408\n",
      "Epoch 30, loss: 2.141330\n",
      "Epoch 31, loss: 2.141286\n",
      "Epoch 32, loss: 2.141228\n",
      "Epoch 33, loss: 2.141152\n",
      "Epoch 34, loss: 2.141107\n",
      "Epoch 35, loss: 2.141045\n",
      "Epoch 36, loss: 2.141008\n",
      "Epoch 37, loss: 2.140942\n",
      "Epoch 38, loss: 2.140865\n",
      "Epoch 39, loss: 2.140804\n",
      "Epoch 40, loss: 2.140744\n",
      "Epoch 41, loss: 2.140686\n",
      "Epoch 42, loss: 2.140627\n",
      "Epoch 43, loss: 2.140583\n",
      "Epoch 44, loss: 2.140511\n",
      "Epoch 45, loss: 2.140459\n",
      "Epoch 46, loss: 2.140402\n",
      "Epoch 47, loss: 2.140343\n",
      "Epoch 48, loss: 2.140281\n",
      "Epoch 49, loss: 2.140220\n",
      "Epoch 50, loss: 2.140173\n",
      "Epoch 51, loss: 2.140104\n",
      "Epoch 52, loss: 2.140042\n",
      "Epoch 53, loss: 2.139989\n",
      "Epoch 54, loss: 2.139938\n",
      "Epoch 55, loss: 2.139887\n",
      "Epoch 56, loss: 2.139815\n",
      "Epoch 57, loss: 2.139760\n",
      "Epoch 58, loss: 2.139704\n",
      "Epoch 59, loss: 2.139646\n",
      "Epoch 60, loss: 2.139593\n",
      "Epoch 61, loss: 2.139530\n",
      "Epoch 62, loss: 2.139477\n",
      "Epoch 63, loss: 2.139425\n",
      "Epoch 64, loss: 2.139388\n",
      "Epoch 65, loss: 2.139306\n",
      "Epoch 66, loss: 2.139254\n",
      "Epoch 67, loss: 2.139206\n",
      "Epoch 68, loss: 2.139139\n",
      "Epoch 69, loss: 2.139071\n",
      "Epoch 70, loss: 2.139024\n",
      "Epoch 71, loss: 2.138975\n",
      "Epoch 72, loss: 2.138913\n",
      "Epoch 73, loss: 2.138851\n",
      "Epoch 74, loss: 2.138817\n",
      "Epoch 75, loss: 2.138740\n",
      "Epoch 76, loss: 2.138693\n",
      "Epoch 77, loss: 2.138646\n",
      "Epoch 78, loss: 2.138593\n",
      "Epoch 79, loss: 2.138539\n",
      "Epoch 80, loss: 2.138489\n",
      "Epoch 81, loss: 2.138421\n",
      "Epoch 82, loss: 2.138350\n",
      "Epoch 83, loss: 2.138297\n",
      "Epoch 84, loss: 2.138259\n",
      "Epoch 85, loss: 2.138209\n",
      "Epoch 86, loss: 2.138147\n",
      "Epoch 87, loss: 2.138091\n",
      "Epoch 88, loss: 2.138032\n",
      "Epoch 89, loss: 2.137973\n",
      "Epoch 90, loss: 2.137937\n",
      "Epoch 91, loss: 2.137876\n",
      "Epoch 92, loss: 2.137824\n",
      "Epoch 93, loss: 2.137745\n",
      "Epoch 94, loss: 2.137711\n",
      "Epoch 95, loss: 2.137674\n",
      "Epoch 96, loss: 2.137606\n",
      "Epoch 97, loss: 2.137551\n",
      "Epoch 98, loss: 2.137485\n",
      "Epoch 99, loss: 2.137440\n",
      "Epoch 100, loss: 2.137372\n",
      "Epoch 101, loss: 2.137326\n",
      "Epoch 102, loss: 2.137265\n",
      "Epoch 103, loss: 2.137223\n",
      "Epoch 104, loss: 2.137162\n",
      "Epoch 105, loss: 2.137122\n",
      "Epoch 106, loss: 2.137070\n",
      "Epoch 107, loss: 2.137016\n",
      "Epoch 108, loss: 2.136946\n",
      "Epoch 109, loss: 2.136893\n",
      "Epoch 110, loss: 2.136842\n",
      "Epoch 111, loss: 2.136802\n",
      "Epoch 112, loss: 2.136744\n",
      "Epoch 113, loss: 2.136711\n",
      "Epoch 114, loss: 2.136640\n",
      "Epoch 115, loss: 2.136588\n",
      "Epoch 116, loss: 2.136534\n",
      "Epoch 117, loss: 2.136510\n",
      "Epoch 118, loss: 2.136437\n",
      "Epoch 119, loss: 2.136380\n",
      "Epoch 120, loss: 2.136326\n",
      "Epoch 121, loss: 2.136277\n",
      "Epoch 122, loss: 2.136212\n",
      "Epoch 123, loss: 2.136165\n",
      "Epoch 124, loss: 2.136120\n",
      "Epoch 125, loss: 2.136079\n",
      "Epoch 126, loss: 2.136014\n",
      "Epoch 127, loss: 2.135969\n",
      "Epoch 128, loss: 2.135912\n",
      "Epoch 129, loss: 2.135855\n",
      "Epoch 130, loss: 2.135813\n",
      "Epoch 131, loss: 2.135753\n",
      "Epoch 132, loss: 2.135721\n",
      "Epoch 133, loss: 2.135660\n",
      "Epoch 134, loss: 2.135601\n",
      "Epoch 135, loss: 2.135545\n",
      "Epoch 136, loss: 2.135510\n",
      "Epoch 137, loss: 2.135471\n",
      "Epoch 138, loss: 2.135423\n",
      "Epoch 139, loss: 2.135351\n",
      "Epoch 140, loss: 2.135304\n",
      "Epoch 141, loss: 2.135266\n",
      "Epoch 142, loss: 2.135201\n",
      "Epoch 143, loss: 2.135147\n",
      "Epoch 144, loss: 2.135115\n",
      "Epoch 145, loss: 2.135043\n",
      "Epoch 146, loss: 2.134988\n",
      "Epoch 147, loss: 2.134942\n",
      "Epoch 148, loss: 2.134899\n",
      "Epoch 149, loss: 2.134847\n",
      "Epoch 150, loss: 2.134796\n",
      "Epoch 151, loss: 2.134753\n",
      "Epoch 152, loss: 2.134708\n",
      "Epoch 153, loss: 2.134649\n",
      "Epoch 154, loss: 2.134630\n",
      "Epoch 155, loss: 2.134604\n",
      "Epoch 156, loss: 2.134508\n",
      "Epoch 157, loss: 2.134455\n",
      "Epoch 158, loss: 2.134402\n",
      "Epoch 159, loss: 2.134365\n",
      "Epoch 160, loss: 2.134307\n",
      "Epoch 161, loss: 2.134256\n",
      "Epoch 162, loss: 2.134225\n",
      "Epoch 163, loss: 2.134172\n",
      "Epoch 164, loss: 2.134096\n",
      "Epoch 165, loss: 2.134066\n",
      "Epoch 166, loss: 2.134018\n",
      "Epoch 167, loss: 2.133956\n",
      "Epoch 168, loss: 2.133932\n",
      "Epoch 169, loss: 2.133855\n",
      "Epoch 170, loss: 2.133823\n",
      "Epoch 171, loss: 2.133769\n",
      "Epoch 172, loss: 2.133709\n",
      "Epoch 173, loss: 2.133669\n",
      "Epoch 174, loss: 2.133629\n",
      "Epoch 175, loss: 2.133574\n",
      "Epoch 176, loss: 2.133512\n",
      "Epoch 177, loss: 2.133484\n",
      "Epoch 178, loss: 2.133438\n",
      "Epoch 179, loss: 2.133384\n",
      "Epoch 180, loss: 2.133336\n",
      "Epoch 181, loss: 2.133301\n",
      "Epoch 182, loss: 2.133237\n",
      "Epoch 183, loss: 2.133181\n",
      "Epoch 184, loss: 2.133155\n",
      "Epoch 185, loss: 2.133131\n",
      "Epoch 186, loss: 2.133058\n",
      "Epoch 187, loss: 2.133009\n",
      "Epoch 188, loss: 2.132969\n",
      "Epoch 189, loss: 2.132914\n",
      "Epoch 190, loss: 2.132864\n",
      "Epoch 191, loss: 2.132827\n",
      "Epoch 192, loss: 2.132767\n",
      "Epoch 193, loss: 2.132714\n",
      "Epoch 194, loss: 2.132665\n",
      "Epoch 195, loss: 2.132620\n",
      "Epoch 196, loss: 2.132579\n",
      "Epoch 197, loss: 2.132545\n",
      "Epoch 198, loss: 2.132479\n",
      "Epoch 199, loss: 2.132449\n",
      "Epoch 0, loss: 2.132105\n",
      "Epoch 1, loss: 2.132052\n",
      "Epoch 2, loss: 2.131991\n",
      "Epoch 3, loss: 2.131955\n",
      "Epoch 4, loss: 2.131906\n",
      "Epoch 5, loss: 2.131859\n",
      "Epoch 6, loss: 2.131829\n",
      "Epoch 7, loss: 2.131775\n",
      "Epoch 8, loss: 2.131718\n",
      "Epoch 9, loss: 2.131663\n",
      "Epoch 10, loss: 2.131619\n",
      "Epoch 11, loss: 2.131584\n",
      "Epoch 12, loss: 2.131544\n",
      "Epoch 13, loss: 2.131493\n",
      "Epoch 14, loss: 2.131435\n",
      "Epoch 15, loss: 2.131393\n",
      "Epoch 16, loss: 2.131348\n",
      "Epoch 17, loss: 2.131298\n",
      "Epoch 18, loss: 2.131262\n",
      "Epoch 19, loss: 2.131221\n",
      "Epoch 20, loss: 2.131171\n",
      "Epoch 21, loss: 2.131131\n",
      "Epoch 22, loss: 2.131065\n",
      "Epoch 23, loss: 2.131020\n",
      "Epoch 24, loss: 2.130976\n",
      "Epoch 25, loss: 2.130928\n",
      "Epoch 26, loss: 2.130886\n",
      "Epoch 27, loss: 2.130831\n",
      "Epoch 28, loss: 2.130804\n",
      "Epoch 29, loss: 2.130743\n",
      "Epoch 30, loss: 2.130700\n",
      "Epoch 31, loss: 2.130668\n",
      "Epoch 32, loss: 2.130608\n",
      "Epoch 33, loss: 2.130576\n",
      "Epoch 34, loss: 2.130514\n",
      "Epoch 35, loss: 2.130492\n",
      "Epoch 36, loss: 2.130436\n",
      "Epoch 37, loss: 2.130388\n",
      "Epoch 38, loss: 2.130343\n",
      "Epoch 39, loss: 2.130295\n",
      "Epoch 40, loss: 2.130251\n",
      "Epoch 41, loss: 2.130200\n",
      "Epoch 42, loss: 2.130162\n",
      "Epoch 43, loss: 2.130116\n",
      "Epoch 44, loss: 2.130069\n",
      "Epoch 45, loss: 2.130032\n",
      "Epoch 46, loss: 2.129987\n",
      "Epoch 47, loss: 2.129947\n",
      "Epoch 48, loss: 2.129885\n",
      "Epoch 49, loss: 2.129844\n",
      "Epoch 50, loss: 2.129809\n",
      "Epoch 51, loss: 2.129760\n",
      "Epoch 52, loss: 2.129711\n",
      "Epoch 53, loss: 2.129670\n",
      "Epoch 54, loss: 2.129625\n",
      "Epoch 55, loss: 2.129576\n",
      "Epoch 56, loss: 2.129533\n",
      "Epoch 57, loss: 2.129490\n",
      "Epoch 58, loss: 2.129464\n",
      "Epoch 59, loss: 2.129407\n",
      "Epoch 60, loss: 2.129374\n",
      "Epoch 61, loss: 2.129308\n",
      "Epoch 62, loss: 2.129256\n",
      "Epoch 63, loss: 2.129244\n",
      "Epoch 64, loss: 2.129170\n",
      "Epoch 65, loss: 2.129139\n",
      "Epoch 66, loss: 2.129091\n",
      "Epoch 67, loss: 2.129054\n",
      "Epoch 68, loss: 2.128997\n",
      "Epoch 69, loss: 2.128965\n",
      "Epoch 70, loss: 2.128936\n",
      "Epoch 71, loss: 2.128876\n",
      "Epoch 72, loss: 2.128840\n",
      "Epoch 73, loss: 2.128779\n",
      "Epoch 74, loss: 2.128756\n",
      "Epoch 75, loss: 2.128710\n",
      "Epoch 76, loss: 2.128680\n",
      "Epoch 77, loss: 2.128614\n",
      "Epoch 78, loss: 2.128571\n",
      "Epoch 79, loss: 2.128529\n",
      "Epoch 80, loss: 2.128488\n",
      "Epoch 81, loss: 2.128447\n",
      "Epoch 82, loss: 2.128409\n",
      "Epoch 83, loss: 2.128361\n",
      "Epoch 84, loss: 2.128329\n",
      "Epoch 85, loss: 2.128279\n",
      "Epoch 86, loss: 2.128231\n",
      "Epoch 87, loss: 2.128183\n",
      "Epoch 88, loss: 2.128157\n",
      "Epoch 89, loss: 2.128113\n",
      "Epoch 90, loss: 2.128052\n",
      "Epoch 91, loss: 2.128049\n",
      "Epoch 92, loss: 2.127974\n",
      "Epoch 93, loss: 2.127939\n",
      "Epoch 94, loss: 2.127904\n",
      "Epoch 95, loss: 2.127866\n",
      "Epoch 96, loss: 2.127811\n",
      "Epoch 97, loss: 2.127778\n",
      "Epoch 98, loss: 2.127742\n",
      "Epoch 99, loss: 2.127685\n",
      "Epoch 100, loss: 2.127639\n",
      "Epoch 101, loss: 2.127598\n",
      "Epoch 102, loss: 2.127563\n",
      "Epoch 103, loss: 2.127509\n",
      "Epoch 104, loss: 2.127478\n",
      "Epoch 105, loss: 2.127434\n",
      "Epoch 106, loss: 2.127383\n",
      "Epoch 107, loss: 2.127332\n",
      "Epoch 108, loss: 2.127308\n",
      "Epoch 109, loss: 2.127280\n",
      "Epoch 110, loss: 2.127220\n",
      "Epoch 111, loss: 2.127188\n",
      "Epoch 112, loss: 2.127158\n",
      "Epoch 113, loss: 2.127120\n",
      "Epoch 114, loss: 2.127058\n",
      "Epoch 115, loss: 2.127022\n",
      "Epoch 116, loss: 2.126951\n",
      "Epoch 117, loss: 2.126953\n",
      "Epoch 118, loss: 2.126896\n",
      "Epoch 119, loss: 2.126835\n",
      "Epoch 120, loss: 2.126808\n",
      "Epoch 121, loss: 2.126777\n",
      "Epoch 122, loss: 2.126731\n",
      "Epoch 123, loss: 2.126687\n",
      "Epoch 124, loss: 2.126658\n",
      "Epoch 125, loss: 2.126624\n",
      "Epoch 126, loss: 2.126569\n",
      "Epoch 127, loss: 2.126529\n",
      "Epoch 128, loss: 2.126479\n",
      "Epoch 129, loss: 2.126441\n",
      "Epoch 130, loss: 2.126410\n",
      "Epoch 131, loss: 2.126371\n",
      "Epoch 132, loss: 2.126339\n",
      "Epoch 133, loss: 2.126277\n",
      "Epoch 134, loss: 2.126253\n",
      "Epoch 135, loss: 2.126212\n",
      "Epoch 136, loss: 2.126158\n",
      "Epoch 137, loss: 2.126119\n",
      "Epoch 138, loss: 2.126074\n",
      "Epoch 139, loss: 2.126045\n",
      "Epoch 140, loss: 2.126011\n",
      "Epoch 141, loss: 2.125966\n",
      "Epoch 142, loss: 2.125936\n",
      "Epoch 143, loss: 2.125898\n",
      "Epoch 144, loss: 2.125847\n",
      "Epoch 145, loss: 2.125806\n",
      "Epoch 146, loss: 2.125754\n",
      "Epoch 147, loss: 2.125737\n",
      "Epoch 148, loss: 2.125681\n",
      "Epoch 149, loss: 2.125643\n",
      "Epoch 150, loss: 2.125593\n",
      "Epoch 151, loss: 2.125565\n",
      "Epoch 152, loss: 2.125514\n",
      "Epoch 153, loss: 2.125487\n",
      "Epoch 154, loss: 2.125448\n",
      "Epoch 155, loss: 2.125408\n",
      "Epoch 156, loss: 2.125349\n",
      "Epoch 157, loss: 2.125327\n",
      "Epoch 158, loss: 2.125282\n",
      "Epoch 159, loss: 2.125241\n",
      "Epoch 160, loss: 2.125187\n",
      "Epoch 161, loss: 2.125179\n",
      "Epoch 162, loss: 2.125137\n",
      "Epoch 163, loss: 2.125079\n",
      "Epoch 164, loss: 2.125046\n",
      "Epoch 165, loss: 2.125001\n",
      "Epoch 166, loss: 2.124973\n",
      "Epoch 167, loss: 2.124927\n",
      "Epoch 168, loss: 2.124900\n",
      "Epoch 169, loss: 2.124861\n",
      "Epoch 170, loss: 2.124841\n",
      "Epoch 171, loss: 2.124776\n",
      "Epoch 172, loss: 2.124736\n",
      "Epoch 173, loss: 2.124720\n",
      "Epoch 174, loss: 2.124654\n",
      "Epoch 175, loss: 2.124647\n",
      "Epoch 176, loss: 2.124586\n",
      "Epoch 177, loss: 2.124550\n",
      "Epoch 178, loss: 2.124505\n",
      "Epoch 179, loss: 2.124481\n",
      "Epoch 180, loss: 2.124421\n",
      "Epoch 181, loss: 2.124404\n",
      "Epoch 182, loss: 2.124368\n",
      "Epoch 183, loss: 2.124328\n",
      "Epoch 184, loss: 2.124284\n",
      "Epoch 185, loss: 2.124250\n",
      "Epoch 186, loss: 2.124208\n",
      "Epoch 187, loss: 2.124150\n",
      "Epoch 188, loss: 2.124114\n",
      "Epoch 189, loss: 2.124076\n",
      "Epoch 190, loss: 2.124041\n",
      "Epoch 191, loss: 2.123992\n",
      "Epoch 192, loss: 2.123995\n",
      "Epoch 193, loss: 2.123928\n",
      "Epoch 194, loss: 2.123900\n",
      "Epoch 195, loss: 2.123869\n",
      "Epoch 196, loss: 2.123810\n",
      "Epoch 197, loss: 2.123783\n",
      "Epoch 198, loss: 2.123734\n",
      "Epoch 199, loss: 2.123703\n",
      "Epoch 0, loss: 2.123650\n",
      "Epoch 1, loss: 2.123584\n",
      "Epoch 2, loss: 2.123579\n",
      "Epoch 3, loss: 2.123518\n",
      "Epoch 4, loss: 2.123477\n",
      "Epoch 5, loss: 2.123438\n",
      "Epoch 6, loss: 2.123390\n",
      "Epoch 7, loss: 2.123360\n",
      "Epoch 8, loss: 2.123352\n",
      "Epoch 9, loss: 2.123297\n",
      "Epoch 10, loss: 2.123260\n",
      "Epoch 11, loss: 2.123207\n",
      "Epoch 12, loss: 2.123172\n",
      "Epoch 13, loss: 2.123150\n",
      "Epoch 14, loss: 2.123091\n",
      "Epoch 15, loss: 2.123056\n",
      "Epoch 16, loss: 2.123040\n",
      "Epoch 17, loss: 2.122999\n",
      "Epoch 18, loss: 2.122969\n",
      "Epoch 19, loss: 2.122930\n",
      "Epoch 20, loss: 2.122870\n",
      "Epoch 21, loss: 2.122845\n",
      "Epoch 22, loss: 2.122800\n",
      "Epoch 23, loss: 2.122764\n",
      "Epoch 24, loss: 2.122736\n",
      "Epoch 25, loss: 2.122686\n",
      "Epoch 26, loss: 2.122653\n",
      "Epoch 27, loss: 2.122620\n",
      "Epoch 28, loss: 2.122585\n",
      "Epoch 29, loss: 2.122564\n",
      "Epoch 30, loss: 2.122513\n",
      "Epoch 31, loss: 2.122474\n",
      "Epoch 32, loss: 2.122432\n",
      "Epoch 33, loss: 2.122392\n",
      "Epoch 34, loss: 2.122370\n",
      "Epoch 35, loss: 2.122337\n",
      "Epoch 36, loss: 2.122297\n",
      "Epoch 37, loss: 2.122246\n",
      "Epoch 38, loss: 2.122218\n",
      "Epoch 39, loss: 2.122175\n",
      "Epoch 40, loss: 2.122147\n",
      "Epoch 41, loss: 2.122099\n",
      "Epoch 42, loss: 2.122073\n",
      "Epoch 43, loss: 2.122046\n",
      "Epoch 44, loss: 2.121999\n",
      "Epoch 45, loss: 2.121992\n",
      "Epoch 46, loss: 2.121922\n",
      "Epoch 47, loss: 2.121900\n",
      "Epoch 48, loss: 2.121858\n",
      "Epoch 49, loss: 2.121832\n",
      "Epoch 50, loss: 2.121770\n",
      "Epoch 51, loss: 2.121763\n",
      "Epoch 52, loss: 2.121708\n",
      "Epoch 53, loss: 2.121679\n",
      "Epoch 54, loss: 2.121648\n",
      "Epoch 55, loss: 2.121616\n",
      "Epoch 56, loss: 2.121571\n",
      "Epoch 57, loss: 2.121541\n",
      "Epoch 58, loss: 2.121504\n",
      "Epoch 59, loss: 2.121468\n",
      "Epoch 60, loss: 2.121439\n",
      "Epoch 61, loss: 2.121391\n",
      "Epoch 62, loss: 2.121335\n",
      "Epoch 63, loss: 2.121317\n",
      "Epoch 64, loss: 2.121286\n",
      "Epoch 65, loss: 2.121266\n",
      "Epoch 66, loss: 2.121220\n",
      "Epoch 67, loss: 2.121178\n",
      "Epoch 68, loss: 2.121160\n",
      "Epoch 69, loss: 2.121101\n",
      "Epoch 70, loss: 2.121071\n",
      "Epoch 71, loss: 2.121034\n",
      "Epoch 72, loss: 2.121003\n",
      "Epoch 73, loss: 2.120969\n",
      "Epoch 74, loss: 2.120935\n",
      "Epoch 75, loss: 2.120906\n",
      "Epoch 76, loss: 2.120854\n",
      "Epoch 77, loss: 2.120826\n",
      "Epoch 78, loss: 2.120798\n",
      "Epoch 79, loss: 2.120767\n",
      "Epoch 80, loss: 2.120740\n",
      "Epoch 81, loss: 2.120691\n",
      "Epoch 82, loss: 2.120646\n",
      "Epoch 83, loss: 2.120617\n",
      "Epoch 84, loss: 2.120575\n",
      "Epoch 85, loss: 2.120556\n",
      "Epoch 86, loss: 2.120504\n",
      "Epoch 87, loss: 2.120486\n",
      "Epoch 88, loss: 2.120447\n",
      "Epoch 89, loss: 2.120417\n",
      "Epoch 90, loss: 2.120388\n",
      "Epoch 91, loss: 2.120338\n",
      "Epoch 92, loss: 2.120313\n",
      "Epoch 93, loss: 2.120288\n",
      "Epoch 94, loss: 2.120243\n",
      "Epoch 95, loss: 2.120197\n",
      "Epoch 96, loss: 2.120180\n",
      "Epoch 97, loss: 2.120143\n",
      "Epoch 98, loss: 2.120107\n",
      "Epoch 99, loss: 2.120060\n",
      "Epoch 100, loss: 2.120027\n",
      "Epoch 101, loss: 2.120005\n",
      "Epoch 102, loss: 2.119956\n",
      "Epoch 103, loss: 2.119934\n",
      "Epoch 104, loss: 2.119890\n",
      "Epoch 105, loss: 2.119854\n",
      "Epoch 106, loss: 2.119821\n",
      "Epoch 107, loss: 2.119802\n",
      "Epoch 108, loss: 2.119754\n",
      "Epoch 109, loss: 2.119726\n",
      "Epoch 110, loss: 2.119693\n",
      "Epoch 111, loss: 2.119648\n",
      "Epoch 112, loss: 2.119614\n",
      "Epoch 113, loss: 2.119596\n",
      "Epoch 114, loss: 2.119565\n",
      "Epoch 115, loss: 2.119524\n",
      "Epoch 116, loss: 2.119484\n",
      "Epoch 117, loss: 2.119446\n",
      "Epoch 118, loss: 2.119414\n",
      "Epoch 119, loss: 2.119399\n",
      "Epoch 120, loss: 2.119337\n",
      "Epoch 121, loss: 2.119313\n",
      "Epoch 122, loss: 2.119269\n",
      "Epoch 123, loss: 2.119245\n",
      "Epoch 124, loss: 2.119218\n",
      "Epoch 125, loss: 2.119170\n",
      "Epoch 126, loss: 2.119142\n",
      "Epoch 127, loss: 2.119116\n",
      "Epoch 128, loss: 2.119067\n",
      "Epoch 129, loss: 2.119045\n",
      "Epoch 130, loss: 2.119010\n",
      "Epoch 131, loss: 2.118970\n",
      "Epoch 132, loss: 2.118951\n",
      "Epoch 133, loss: 2.118916\n",
      "Epoch 134, loss: 2.118868\n",
      "Epoch 135, loss: 2.118856\n",
      "Epoch 136, loss: 2.118830\n",
      "Epoch 137, loss: 2.118772\n",
      "Epoch 138, loss: 2.118750\n",
      "Epoch 139, loss: 2.118705\n",
      "Epoch 140, loss: 2.118681\n",
      "Epoch 141, loss: 2.118665\n",
      "Epoch 142, loss: 2.118615\n",
      "Epoch 143, loss: 2.118583\n",
      "Epoch 144, loss: 2.118550\n",
      "Epoch 145, loss: 2.118527\n",
      "Epoch 146, loss: 2.118484\n",
      "Epoch 147, loss: 2.118448\n",
      "Epoch 148, loss: 2.118418\n",
      "Epoch 149, loss: 2.118383\n",
      "Epoch 150, loss: 2.118341\n",
      "Epoch 151, loss: 2.118322\n",
      "Epoch 152, loss: 2.118271\n",
      "Epoch 153, loss: 2.118258\n",
      "Epoch 154, loss: 2.118229\n",
      "Epoch 155, loss: 2.118176\n",
      "Epoch 156, loss: 2.118146\n",
      "Epoch 157, loss: 2.118121\n",
      "Epoch 158, loss: 2.118107\n",
      "Epoch 159, loss: 2.118056\n",
      "Epoch 160, loss: 2.118038\n",
      "Epoch 161, loss: 2.117979\n",
      "Epoch 162, loss: 2.117942\n",
      "Epoch 163, loss: 2.117928\n",
      "Epoch 164, loss: 2.117915\n",
      "Epoch 165, loss: 2.117867\n",
      "Epoch 166, loss: 2.117829\n",
      "Epoch 167, loss: 2.117808\n",
      "Epoch 168, loss: 2.117759\n",
      "Epoch 169, loss: 2.117751\n",
      "Epoch 170, loss: 2.117688\n",
      "Epoch 171, loss: 2.117686\n",
      "Epoch 172, loss: 2.117626\n",
      "Epoch 173, loss: 2.117588\n",
      "Epoch 174, loss: 2.117565\n",
      "Epoch 175, loss: 2.117558\n",
      "Epoch 176, loss: 2.117507\n",
      "Epoch 177, loss: 2.117478\n",
      "Epoch 178, loss: 2.117431\n",
      "Epoch 179, loss: 2.117417\n",
      "Epoch 180, loss: 2.117388\n",
      "Epoch 181, loss: 2.117355\n",
      "Epoch 182, loss: 2.117316\n",
      "Epoch 183, loss: 2.117278\n",
      "Epoch 184, loss: 2.117250\n",
      "Epoch 185, loss: 2.117220\n",
      "Epoch 186, loss: 2.117185\n",
      "Epoch 187, loss: 2.117158\n",
      "Epoch 188, loss: 2.117118\n",
      "Epoch 189, loss: 2.117093\n",
      "Epoch 190, loss: 2.117055\n",
      "Epoch 191, loss: 2.117027\n",
      "Epoch 192, loss: 2.116989\n",
      "Epoch 193, loss: 2.116985\n",
      "Epoch 194, loss: 2.116921\n",
      "Epoch 195, loss: 2.116910\n",
      "Epoch 196, loss: 2.116866\n",
      "Epoch 197, loss: 2.116835\n",
      "Epoch 198, loss: 2.116822\n",
      "Epoch 199, loss: 2.116781\n",
      "Epoch 0, loss: 2.117074\n",
      "Epoch 1, loss: 2.117072\n",
      "Epoch 2, loss: 2.117068\n",
      "Epoch 3, loss: 2.117064\n",
      "Epoch 4, loss: 2.117061\n",
      "Epoch 5, loss: 2.117060\n",
      "Epoch 6, loss: 2.117055\n",
      "Epoch 7, loss: 2.117053\n",
      "Epoch 8, loss: 2.117049\n",
      "Epoch 9, loss: 2.117047\n",
      "Epoch 10, loss: 2.117044\n",
      "Epoch 11, loss: 2.117039\n",
      "Epoch 12, loss: 2.117037\n",
      "Epoch 13, loss: 2.117033\n",
      "Epoch 14, loss: 2.117030\n",
      "Epoch 15, loss: 2.117028\n",
      "Epoch 16, loss: 2.117024\n",
      "Epoch 17, loss: 2.117022\n",
      "Epoch 18, loss: 2.117018\n",
      "Epoch 19, loss: 2.117016\n",
      "Epoch 20, loss: 2.117013\n",
      "Epoch 21, loss: 2.117010\n",
      "Epoch 22, loss: 2.117006\n",
      "Epoch 23, loss: 2.117004\n",
      "Epoch 24, loss: 2.117000\n",
      "Epoch 25, loss: 2.116996\n",
      "Epoch 26, loss: 2.116994\n",
      "Epoch 27, loss: 2.116990\n",
      "Epoch 28, loss: 2.116987\n",
      "Epoch 29, loss: 2.116986\n",
      "Epoch 30, loss: 2.116981\n",
      "Epoch 31, loss: 2.116979\n",
      "Epoch 32, loss: 2.116976\n",
      "Epoch 33, loss: 2.116973\n",
      "Epoch 34, loss: 2.116970\n",
      "Epoch 35, loss: 2.116965\n",
      "Epoch 36, loss: 2.116961\n",
      "Epoch 37, loss: 2.116961\n",
      "Epoch 38, loss: 2.116956\n",
      "Epoch 39, loss: 2.116953\n",
      "Epoch 40, loss: 2.116949\n",
      "Epoch 41, loss: 2.116947\n",
      "Epoch 42, loss: 2.116945\n",
      "Epoch 43, loss: 2.116941\n",
      "Epoch 44, loss: 2.116939\n",
      "Epoch 45, loss: 2.116934\n",
      "Epoch 46, loss: 2.116932\n",
      "Epoch 47, loss: 2.116928\n",
      "Epoch 48, loss: 2.116926\n",
      "Epoch 49, loss: 2.116922\n",
      "Epoch 50, loss: 2.116921\n",
      "Epoch 51, loss: 2.116917\n",
      "Epoch 52, loss: 2.116914\n",
      "Epoch 53, loss: 2.116911\n",
      "Epoch 54, loss: 2.116907\n",
      "Epoch 55, loss: 2.116904\n",
      "Epoch 56, loss: 2.116903\n",
      "Epoch 57, loss: 2.116898\n",
      "Epoch 58, loss: 2.116896\n",
      "Epoch 59, loss: 2.116893\n",
      "Epoch 60, loss: 2.116891\n",
      "Epoch 61, loss: 2.116887\n",
      "Epoch 62, loss: 2.116882\n",
      "Epoch 63, loss: 2.116879\n",
      "Epoch 64, loss: 2.116877\n",
      "Epoch 65, loss: 2.116874\n",
      "Epoch 66, loss: 2.116872\n",
      "Epoch 67, loss: 2.116868\n",
      "Epoch 68, loss: 2.116864\n",
      "Epoch 69, loss: 2.116861\n",
      "Epoch 70, loss: 2.116859\n",
      "Epoch 71, loss: 2.116856\n",
      "Epoch 72, loss: 2.116851\n",
      "Epoch 73, loss: 2.116850\n",
      "Epoch 74, loss: 2.116848\n",
      "Epoch 75, loss: 2.116843\n",
      "Epoch 76, loss: 2.116841\n",
      "Epoch 77, loss: 2.116839\n",
      "Epoch 78, loss: 2.116834\n",
      "Epoch 79, loss: 2.116829\n",
      "Epoch 80, loss: 2.116828\n",
      "Epoch 81, loss: 2.116824\n",
      "Epoch 82, loss: 2.116822\n",
      "Epoch 83, loss: 2.116819\n",
      "Epoch 84, loss: 2.116815\n",
      "Epoch 85, loss: 2.116812\n",
      "Epoch 86, loss: 2.116809\n",
      "Epoch 87, loss: 2.116808\n",
      "Epoch 88, loss: 2.116803\n",
      "Epoch 89, loss: 2.116800\n",
      "Epoch 90, loss: 2.116797\n",
      "Epoch 91, loss: 2.116794\n",
      "Epoch 92, loss: 2.116790\n",
      "Epoch 93, loss: 2.116788\n",
      "Epoch 94, loss: 2.116785\n",
      "Epoch 95, loss: 2.116781\n",
      "Epoch 96, loss: 2.116779\n",
      "Epoch 97, loss: 2.116778\n",
      "Epoch 98, loss: 2.116774\n",
      "Epoch 99, loss: 2.116769\n",
      "Epoch 100, loss: 2.116769\n",
      "Epoch 101, loss: 2.116763\n",
      "Epoch 102, loss: 2.116760\n",
      "Epoch 103, loss: 2.116758\n",
      "Epoch 104, loss: 2.116756\n",
      "Epoch 105, loss: 2.116751\n",
      "Epoch 106, loss: 2.116749\n",
      "Epoch 107, loss: 2.116747\n",
      "Epoch 108, loss: 2.116743\n",
      "Epoch 109, loss: 2.116739\n",
      "Epoch 110, loss: 2.116736\n",
      "Epoch 111, loss: 2.116733\n",
      "Epoch 112, loss: 2.116730\n",
      "Epoch 113, loss: 2.116728\n",
      "Epoch 114, loss: 2.116723\n",
      "Epoch 115, loss: 2.116721\n",
      "Epoch 116, loss: 2.116718\n",
      "Epoch 117, loss: 2.116715\n",
      "Epoch 118, loss: 2.116711\n",
      "Epoch 119, loss: 2.116709\n",
      "Epoch 120, loss: 2.116707\n",
      "Epoch 121, loss: 2.116701\n",
      "Epoch 122, loss: 2.116700\n",
      "Epoch 123, loss: 2.116697\n",
      "Epoch 124, loss: 2.116694\n",
      "Epoch 125, loss: 2.116690\n",
      "Epoch 126, loss: 2.116686\n",
      "Epoch 127, loss: 2.116684\n",
      "Epoch 128, loss: 2.116682\n",
      "Epoch 129, loss: 2.116679\n",
      "Epoch 130, loss: 2.116675\n",
      "Epoch 131, loss: 2.116672\n",
      "Epoch 132, loss: 2.116669\n",
      "Epoch 133, loss: 2.116666\n",
      "Epoch 134, loss: 2.116662\n",
      "Epoch 135, loss: 2.116659\n",
      "Epoch 136, loss: 2.116658\n",
      "Epoch 137, loss: 2.116655\n",
      "Epoch 138, loss: 2.116652\n",
      "Epoch 139, loss: 2.116647\n",
      "Epoch 140, loss: 2.116645\n",
      "Epoch 141, loss: 2.116642\n",
      "Epoch 142, loss: 2.116639\n",
      "Epoch 143, loss: 2.116636\n",
      "Epoch 144, loss: 2.116631\n",
      "Epoch 145, loss: 2.116628\n",
      "Epoch 146, loss: 2.116626\n",
      "Epoch 147, loss: 2.116625\n",
      "Epoch 148, loss: 2.116619\n",
      "Epoch 149, loss: 2.116618\n",
      "Epoch 150, loss: 2.116614\n",
      "Epoch 151, loss: 2.116612\n",
      "Epoch 152, loss: 2.116609\n",
      "Epoch 153, loss: 2.116606\n",
      "Epoch 154, loss: 2.116603\n",
      "Epoch 155, loss: 2.116600\n",
      "Epoch 156, loss: 2.116595\n",
      "Epoch 157, loss: 2.116593\n",
      "Epoch 158, loss: 2.116589\n",
      "Epoch 159, loss: 2.116587\n",
      "Epoch 160, loss: 2.116587\n",
      "Epoch 161, loss: 2.116582\n",
      "Epoch 162, loss: 2.116578\n",
      "Epoch 163, loss: 2.116574\n",
      "Epoch 164, loss: 2.116572\n",
      "Epoch 165, loss: 2.116569\n",
      "Epoch 166, loss: 2.116565\n",
      "Epoch 167, loss: 2.116563\n",
      "Epoch 168, loss: 2.116561\n",
      "Epoch 169, loss: 2.116557\n",
      "Epoch 170, loss: 2.116554\n",
      "Epoch 171, loss: 2.116552\n",
      "Epoch 172, loss: 2.116550\n",
      "Epoch 173, loss: 2.116543\n",
      "Epoch 174, loss: 2.116542\n",
      "Epoch 175, loss: 2.116538\n",
      "Epoch 176, loss: 2.116534\n",
      "Epoch 177, loss: 2.116532\n",
      "Epoch 178, loss: 2.116528\n",
      "Epoch 179, loss: 2.116526\n",
      "Epoch 180, loss: 2.116524\n",
      "Epoch 181, loss: 2.116521\n",
      "Epoch 182, loss: 2.116520\n",
      "Epoch 183, loss: 2.116516\n",
      "Epoch 184, loss: 2.116511\n",
      "Epoch 185, loss: 2.116508\n",
      "Epoch 186, loss: 2.116505\n",
      "Epoch 187, loss: 2.116502\n",
      "Epoch 188, loss: 2.116499\n",
      "Epoch 189, loss: 2.116497\n",
      "Epoch 190, loss: 2.116494\n",
      "Epoch 191, loss: 2.116489\n",
      "Epoch 192, loss: 2.116489\n",
      "Epoch 193, loss: 2.116484\n",
      "Epoch 194, loss: 2.116481\n",
      "Epoch 195, loss: 2.116478\n",
      "Epoch 196, loss: 2.116474\n",
      "Epoch 197, loss: 2.116471\n",
      "Epoch 198, loss: 2.116470\n",
      "Epoch 199, loss: 2.116465\n",
      "Epoch 0, loss: 2.116036\n",
      "Epoch 1, loss: 2.116034\n",
      "Epoch 2, loss: 2.116029\n",
      "Epoch 3, loss: 2.116026\n",
      "Epoch 4, loss: 2.116023\n",
      "Epoch 5, loss: 2.116020\n",
      "Epoch 6, loss: 2.116016\n",
      "Epoch 7, loss: 2.116013\n",
      "Epoch 8, loss: 2.116012\n",
      "Epoch 9, loss: 2.116007\n",
      "Epoch 10, loss: 2.116006\n",
      "Epoch 11, loss: 2.116001\n",
      "Epoch 12, loss: 2.115999\n",
      "Epoch 13, loss: 2.115996\n",
      "Epoch 14, loss: 2.115990\n",
      "Epoch 15, loss: 2.115988\n",
      "Epoch 16, loss: 2.115985\n",
      "Epoch 17, loss: 2.115984\n",
      "Epoch 18, loss: 2.115979\n",
      "Epoch 19, loss: 2.115979\n",
      "Epoch 20, loss: 2.115975\n",
      "Epoch 21, loss: 2.115972\n",
      "Epoch 22, loss: 2.115967\n",
      "Epoch 23, loss: 2.115967\n",
      "Epoch 24, loss: 2.115962\n",
      "Epoch 25, loss: 2.115960\n",
      "Epoch 26, loss: 2.115957\n",
      "Epoch 27, loss: 2.115952\n",
      "Epoch 28, loss: 2.115949\n",
      "Epoch 29, loss: 2.115946\n",
      "Epoch 30, loss: 2.115944\n",
      "Epoch 31, loss: 2.115940\n",
      "Epoch 32, loss: 2.115937\n",
      "Epoch 33, loss: 2.115933\n",
      "Epoch 34, loss: 2.115930\n",
      "Epoch 35, loss: 2.115928\n",
      "Epoch 36, loss: 2.115925\n",
      "Epoch 37, loss: 2.115921\n",
      "Epoch 38, loss: 2.115920\n",
      "Epoch 39, loss: 2.115915\n",
      "Epoch 40, loss: 2.115912\n",
      "Epoch 41, loss: 2.115909\n",
      "Epoch 42, loss: 2.115906\n",
      "Epoch 43, loss: 2.115902\n",
      "Epoch 44, loss: 2.115899\n",
      "Epoch 45, loss: 2.115899\n",
      "Epoch 46, loss: 2.115892\n",
      "Epoch 47, loss: 2.115891\n",
      "Epoch 48, loss: 2.115887\n",
      "Epoch 49, loss: 2.115884\n",
      "Epoch 50, loss: 2.115882\n",
      "Epoch 51, loss: 2.115878\n",
      "Epoch 52, loss: 2.115875\n",
      "Epoch 53, loss: 2.115872\n",
      "Epoch 54, loss: 2.115870\n",
      "Epoch 55, loss: 2.115865\n",
      "Epoch 56, loss: 2.115863\n",
      "Epoch 57, loss: 2.115859\n",
      "Epoch 58, loss: 2.115855\n",
      "Epoch 59, loss: 2.115854\n",
      "Epoch 60, loss: 2.115851\n",
      "Epoch 61, loss: 2.115848\n",
      "Epoch 62, loss: 2.115845\n",
      "Epoch 63, loss: 2.115844\n",
      "Epoch 64, loss: 2.115839\n",
      "Epoch 65, loss: 2.115835\n",
      "Epoch 66, loss: 2.115832\n",
      "Epoch 67, loss: 2.115828\n",
      "Epoch 68, loss: 2.115825\n",
      "Epoch 69, loss: 2.115822\n",
      "Epoch 70, loss: 2.115820\n",
      "Epoch 71, loss: 2.115818\n",
      "Epoch 72, loss: 2.115814\n",
      "Epoch 73, loss: 2.115810\n",
      "Epoch 74, loss: 2.115807\n",
      "Epoch 75, loss: 2.115803\n",
      "Epoch 76, loss: 2.115803\n",
      "Epoch 77, loss: 2.115799\n",
      "Epoch 78, loss: 2.115795\n",
      "Epoch 79, loss: 2.115792\n",
      "Epoch 80, loss: 2.115788\n",
      "Epoch 81, loss: 2.115786\n",
      "Epoch 82, loss: 2.115784\n",
      "Epoch 83, loss: 2.115778\n",
      "Epoch 84, loss: 2.115778\n",
      "Epoch 85, loss: 2.115773\n",
      "Epoch 86, loss: 2.115771\n",
      "Epoch 87, loss: 2.115768\n",
      "Epoch 88, loss: 2.115766\n",
      "Epoch 89, loss: 2.115762\n",
      "Epoch 90, loss: 2.115759\n",
      "Epoch 91, loss: 2.115756\n",
      "Epoch 92, loss: 2.115753\n",
      "Epoch 93, loss: 2.115748\n",
      "Epoch 94, loss: 2.115747\n",
      "Epoch 95, loss: 2.115743\n",
      "Epoch 96, loss: 2.115740\n",
      "Epoch 97, loss: 2.115737\n",
      "Epoch 98, loss: 2.115734\n",
      "Epoch 99, loss: 2.115730\n",
      "Epoch 100, loss: 2.115727\n",
      "Epoch 101, loss: 2.115723\n",
      "Epoch 102, loss: 2.115722\n",
      "Epoch 103, loss: 2.115718\n",
      "Epoch 104, loss: 2.115716\n",
      "Epoch 105, loss: 2.115712\n",
      "Epoch 106, loss: 2.115711\n",
      "Epoch 107, loss: 2.115708\n",
      "Epoch 108, loss: 2.115702\n",
      "Epoch 109, loss: 2.115702\n",
      "Epoch 110, loss: 2.115700\n",
      "Epoch 111, loss: 2.115694\n",
      "Epoch 112, loss: 2.115692\n",
      "Epoch 113, loss: 2.115689\n",
      "Epoch 114, loss: 2.115686\n",
      "Epoch 115, loss: 2.115684\n",
      "Epoch 116, loss: 2.115680\n",
      "Epoch 117, loss: 2.115675\n",
      "Epoch 118, loss: 2.115672\n",
      "Epoch 119, loss: 2.115669\n",
      "Epoch 120, loss: 2.115666\n",
      "Epoch 121, loss: 2.115666\n",
      "Epoch 122, loss: 2.115662\n",
      "Epoch 123, loss: 2.115657\n",
      "Epoch 124, loss: 2.115655\n",
      "Epoch 125, loss: 2.115653\n",
      "Epoch 126, loss: 2.115649\n",
      "Epoch 127, loss: 2.115644\n",
      "Epoch 128, loss: 2.115642\n",
      "Epoch 129, loss: 2.115639\n",
      "Epoch 130, loss: 2.115637\n",
      "Epoch 131, loss: 2.115634\n",
      "Epoch 132, loss: 2.115629\n",
      "Epoch 133, loss: 2.115626\n",
      "Epoch 134, loss: 2.115622\n",
      "Epoch 135, loss: 2.115620\n",
      "Epoch 136, loss: 2.115617\n",
      "Epoch 137, loss: 2.115613\n",
      "Epoch 138, loss: 2.115611\n",
      "Epoch 139, loss: 2.115607\n",
      "Epoch 140, loss: 2.115605\n",
      "Epoch 141, loss: 2.115604\n",
      "Epoch 142, loss: 2.115599\n",
      "Epoch 143, loss: 2.115597\n",
      "Epoch 144, loss: 2.115593\n",
      "Epoch 145, loss: 2.115590\n",
      "Epoch 146, loss: 2.115587\n",
      "Epoch 147, loss: 2.115586\n",
      "Epoch 148, loss: 2.115581\n",
      "Epoch 149, loss: 2.115578\n",
      "Epoch 150, loss: 2.115574\n",
      "Epoch 151, loss: 2.115572\n",
      "Epoch 152, loss: 2.115569\n",
      "Epoch 153, loss: 2.115565\n",
      "Epoch 154, loss: 2.115563\n",
      "Epoch 155, loss: 2.115560\n",
      "Epoch 156, loss: 2.115557\n",
      "Epoch 157, loss: 2.115554\n",
      "Epoch 158, loss: 2.115550\n",
      "Epoch 159, loss: 2.115548\n",
      "Epoch 160, loss: 2.115544\n",
      "Epoch 161, loss: 2.115542\n",
      "Epoch 162, loss: 2.115538\n",
      "Epoch 163, loss: 2.115536\n",
      "Epoch 164, loss: 2.115534\n",
      "Epoch 165, loss: 2.115527\n",
      "Epoch 166, loss: 2.115526\n",
      "Epoch 167, loss: 2.115524\n",
      "Epoch 168, loss: 2.115521\n",
      "Epoch 169, loss: 2.115518\n",
      "Epoch 170, loss: 2.115512\n",
      "Epoch 171, loss: 2.115511\n",
      "Epoch 172, loss: 2.115508\n",
      "Epoch 173, loss: 2.115505\n",
      "Epoch 174, loss: 2.115501\n",
      "Epoch 175, loss: 2.115498\n",
      "Epoch 176, loss: 2.115496\n",
      "Epoch 177, loss: 2.115493\n",
      "Epoch 178, loss: 2.115489\n",
      "Epoch 179, loss: 2.115486\n",
      "Epoch 180, loss: 2.115483\n",
      "Epoch 181, loss: 2.115480\n",
      "Epoch 182, loss: 2.115479\n",
      "Epoch 183, loss: 2.115473\n",
      "Epoch 184, loss: 2.115472\n",
      "Epoch 185, loss: 2.115468\n",
      "Epoch 186, loss: 2.115466\n",
      "Epoch 187, loss: 2.115463\n",
      "Epoch 188, loss: 2.115460\n",
      "Epoch 189, loss: 2.115455\n",
      "Epoch 190, loss: 2.115452\n",
      "Epoch 191, loss: 2.115451\n",
      "Epoch 192, loss: 2.115445\n",
      "Epoch 193, loss: 2.115443\n",
      "Epoch 194, loss: 2.115439\n",
      "Epoch 195, loss: 2.115438\n",
      "Epoch 196, loss: 2.115435\n",
      "Epoch 197, loss: 2.115432\n",
      "Epoch 198, loss: 2.115428\n",
      "Epoch 199, loss: 2.115426\n",
      "Epoch 0, loss: 2.115378\n",
      "Epoch 1, loss: 2.115377\n",
      "Epoch 2, loss: 2.115373\n",
      "Epoch 3, loss: 2.115370\n",
      "Epoch 4, loss: 2.115367\n",
      "Epoch 5, loss: 2.115367\n",
      "Epoch 6, loss: 2.115361\n",
      "Epoch 7, loss: 2.115358\n",
      "Epoch 8, loss: 2.115355\n",
      "Epoch 9, loss: 2.115352\n",
      "Epoch 10, loss: 2.115349\n",
      "Epoch 11, loss: 2.115345\n",
      "Epoch 12, loss: 2.115344\n",
      "Epoch 13, loss: 2.115339\n",
      "Epoch 14, loss: 2.115335\n",
      "Epoch 15, loss: 2.115334\n",
      "Epoch 16, loss: 2.115331\n",
      "Epoch 17, loss: 2.115327\n",
      "Epoch 18, loss: 2.115323\n",
      "Epoch 19, loss: 2.115322\n",
      "Epoch 20, loss: 2.115318\n",
      "Epoch 21, loss: 2.115315\n",
      "Epoch 22, loss: 2.115312\n",
      "Epoch 23, loss: 2.115308\n",
      "Epoch 24, loss: 2.115306\n",
      "Epoch 25, loss: 2.115303\n",
      "Epoch 26, loss: 2.115301\n",
      "Epoch 27, loss: 2.115298\n",
      "Epoch 28, loss: 2.115293\n",
      "Epoch 29, loss: 2.115290\n",
      "Epoch 30, loss: 2.115288\n",
      "Epoch 31, loss: 2.115285\n",
      "Epoch 32, loss: 2.115283\n",
      "Epoch 33, loss: 2.115279\n",
      "Epoch 34, loss: 2.115276\n",
      "Epoch 35, loss: 2.115273\n",
      "Epoch 36, loss: 2.115271\n",
      "Epoch 37, loss: 2.115266\n",
      "Epoch 38, loss: 2.115262\n",
      "Epoch 39, loss: 2.115260\n",
      "Epoch 40, loss: 2.115259\n",
      "Epoch 41, loss: 2.115255\n",
      "Epoch 42, loss: 2.115252\n",
      "Epoch 43, loss: 2.115248\n",
      "Epoch 44, loss: 2.115248\n",
      "Epoch 45, loss: 2.115243\n",
      "Epoch 46, loss: 2.115239\n",
      "Epoch 47, loss: 2.115236\n",
      "Epoch 48, loss: 2.115233\n",
      "Epoch 49, loss: 2.115231\n",
      "Epoch 50, loss: 2.115228\n",
      "Epoch 51, loss: 2.115225\n",
      "Epoch 52, loss: 2.115221\n",
      "Epoch 53, loss: 2.115218\n",
      "Epoch 54, loss: 2.115214\n",
      "Epoch 55, loss: 2.115214\n",
      "Epoch 56, loss: 2.115209\n",
      "Epoch 57, loss: 2.115206\n",
      "Epoch 58, loss: 2.115202\n",
      "Epoch 59, loss: 2.115198\n",
      "Epoch 60, loss: 2.115196\n",
      "Epoch 61, loss: 2.115193\n",
      "Epoch 62, loss: 2.115191\n",
      "Epoch 63, loss: 2.115189\n",
      "Epoch 64, loss: 2.115187\n",
      "Epoch 65, loss: 2.115183\n",
      "Epoch 66, loss: 2.115179\n",
      "Epoch 67, loss: 2.115175\n",
      "Epoch 68, loss: 2.115172\n",
      "Epoch 69, loss: 2.115169\n",
      "Epoch 70, loss: 2.115168\n",
      "Epoch 71, loss: 2.115164\n",
      "Epoch 72, loss: 2.115159\n",
      "Epoch 73, loss: 2.115157\n",
      "Epoch 74, loss: 2.115154\n",
      "Epoch 75, loss: 2.115152\n",
      "Epoch 76, loss: 2.115148\n",
      "Epoch 77, loss: 2.115145\n",
      "Epoch 78, loss: 2.115141\n",
      "Epoch 79, loss: 2.115138\n",
      "Epoch 80, loss: 2.115138\n",
      "Epoch 81, loss: 2.115134\n",
      "Epoch 82, loss: 2.115131\n",
      "Epoch 83, loss: 2.115127\n",
      "Epoch 84, loss: 2.115125\n",
      "Epoch 85, loss: 2.115121\n",
      "Epoch 86, loss: 2.115119\n",
      "Epoch 87, loss: 2.115116\n",
      "Epoch 88, loss: 2.115113\n",
      "Epoch 89, loss: 2.115110\n",
      "Epoch 90, loss: 2.115107\n",
      "Epoch 91, loss: 2.115102\n",
      "Epoch 92, loss: 2.115101\n",
      "Epoch 93, loss: 2.115098\n",
      "Epoch 94, loss: 2.115094\n",
      "Epoch 95, loss: 2.115090\n",
      "Epoch 96, loss: 2.115089\n",
      "Epoch 97, loss: 2.115086\n",
      "Epoch 98, loss: 2.115083\n",
      "Epoch 99, loss: 2.115078\n",
      "Epoch 100, loss: 2.115077\n",
      "Epoch 101, loss: 2.115073\n",
      "Epoch 102, loss: 2.115069\n",
      "Epoch 103, loss: 2.115068\n",
      "Epoch 104, loss: 2.115067\n",
      "Epoch 105, loss: 2.115060\n",
      "Epoch 106, loss: 2.115059\n",
      "Epoch 107, loss: 2.115054\n",
      "Epoch 108, loss: 2.115052\n",
      "Epoch 109, loss: 2.115048\n",
      "Epoch 110, loss: 2.115045\n",
      "Epoch 111, loss: 2.115043\n",
      "Epoch 112, loss: 2.115040\n",
      "Epoch 113, loss: 2.115037\n",
      "Epoch 114, loss: 2.115035\n",
      "Epoch 115, loss: 2.115030\n",
      "Epoch 116, loss: 2.115028\n",
      "Epoch 117, loss: 2.115025\n",
      "Epoch 118, loss: 2.115023\n",
      "Epoch 119, loss: 2.115017\n",
      "Epoch 120, loss: 2.115018\n",
      "Epoch 121, loss: 2.115011\n",
      "Epoch 122, loss: 2.115010\n",
      "Epoch 123, loss: 2.115006\n",
      "Epoch 124, loss: 2.115004\n",
      "Epoch 125, loss: 2.115000\n",
      "Epoch 126, loss: 2.114997\n",
      "Epoch 127, loss: 2.114993\n",
      "Epoch 128, loss: 2.114992\n",
      "Epoch 129, loss: 2.114989\n",
      "Epoch 130, loss: 2.114988\n",
      "Epoch 131, loss: 2.114985\n",
      "Epoch 132, loss: 2.114980\n",
      "Epoch 133, loss: 2.114975\n",
      "Epoch 134, loss: 2.114972\n",
      "Epoch 135, loss: 2.114970\n",
      "Epoch 136, loss: 2.114968\n",
      "Epoch 137, loss: 2.114965\n",
      "Epoch 138, loss: 2.114961\n",
      "Epoch 139, loss: 2.114958\n",
      "Epoch 140, loss: 2.114956\n",
      "Epoch 141, loss: 2.114953\n",
      "Epoch 142, loss: 2.114948\n",
      "Epoch 143, loss: 2.114947\n",
      "Epoch 144, loss: 2.114944\n",
      "Epoch 145, loss: 2.114940\n",
      "Epoch 146, loss: 2.114937\n",
      "Epoch 147, loss: 2.114935\n",
      "Epoch 148, loss: 2.114931\n",
      "Epoch 149, loss: 2.114928\n",
      "Epoch 150, loss: 2.114924\n",
      "Epoch 151, loss: 2.114922\n",
      "Epoch 152, loss: 2.114920\n",
      "Epoch 153, loss: 2.114916\n",
      "Epoch 154, loss: 2.114913\n",
      "Epoch 155, loss: 2.114910\n",
      "Epoch 156, loss: 2.114907\n",
      "Epoch 157, loss: 2.114904\n",
      "Epoch 158, loss: 2.114901\n",
      "Epoch 159, loss: 2.114897\n",
      "Epoch 160, loss: 2.114894\n",
      "Epoch 161, loss: 2.114891\n",
      "Epoch 162, loss: 2.114889\n",
      "Epoch 163, loss: 2.114889\n",
      "Epoch 164, loss: 2.114884\n",
      "Epoch 165, loss: 2.114880\n",
      "Epoch 166, loss: 2.114877\n",
      "Epoch 167, loss: 2.114873\n",
      "Epoch 168, loss: 2.114871\n",
      "Epoch 169, loss: 2.114868\n",
      "Epoch 170, loss: 2.114866\n",
      "Epoch 171, loss: 2.114863\n",
      "Epoch 172, loss: 2.114859\n",
      "Epoch 173, loss: 2.114855\n",
      "Epoch 174, loss: 2.114853\n",
      "Epoch 175, loss: 2.114851\n",
      "Epoch 176, loss: 2.114847\n",
      "Epoch 177, loss: 2.114844\n",
      "Epoch 178, loss: 2.114842\n",
      "Epoch 179, loss: 2.114837\n",
      "Epoch 180, loss: 2.114835\n",
      "Epoch 181, loss: 2.114833\n",
      "Epoch 182, loss: 2.114828\n",
      "Epoch 183, loss: 2.114825\n",
      "Epoch 184, loss: 2.114823\n",
      "Epoch 185, loss: 2.114819\n",
      "Epoch 186, loss: 2.114816\n",
      "Epoch 187, loss: 2.114814\n",
      "Epoch 188, loss: 2.114812\n",
      "Epoch 189, loss: 2.114808\n",
      "Epoch 190, loss: 2.114804\n",
      "Epoch 191, loss: 2.114803\n",
      "Epoch 192, loss: 2.114800\n",
      "Epoch 193, loss: 2.114796\n",
      "Epoch 194, loss: 2.114793\n",
      "Epoch 195, loss: 2.114790\n",
      "Epoch 196, loss: 2.114787\n",
      "Epoch 197, loss: 2.114782\n",
      "Epoch 198, loss: 2.114781\n",
      "Epoch 199, loss: 2.114777\n",
      "Epoch 0, loss: 2.115245\n",
      "Epoch 1, loss: 2.115244\n",
      "Epoch 2, loss: 2.115244\n",
      "Epoch 3, loss: 2.115244\n",
      "Epoch 4, loss: 2.115244\n",
      "Epoch 5, loss: 2.115243\n",
      "Epoch 6, loss: 2.115243\n",
      "Epoch 7, loss: 2.115243\n",
      "Epoch 8, loss: 2.115242\n",
      "Epoch 9, loss: 2.115242\n",
      "Epoch 10, loss: 2.115242\n",
      "Epoch 11, loss: 2.115241\n",
      "Epoch 12, loss: 2.115241\n",
      "Epoch 13, loss: 2.115241\n",
      "Epoch 14, loss: 2.115241\n",
      "Epoch 15, loss: 2.115240\n",
      "Epoch 16, loss: 2.115240\n",
      "Epoch 17, loss: 2.115240\n",
      "Epoch 18, loss: 2.115240\n",
      "Epoch 19, loss: 2.115239\n",
      "Epoch 20, loss: 2.115239\n",
      "Epoch 21, loss: 2.115238\n",
      "Epoch 22, loss: 2.115238\n",
      "Epoch 23, loss: 2.115238\n",
      "Epoch 24, loss: 2.115237\n",
      "Epoch 25, loss: 2.115237\n",
      "Epoch 26, loss: 2.115237\n",
      "Epoch 27, loss: 2.115237\n",
      "Epoch 28, loss: 2.115237\n",
      "Epoch 29, loss: 2.115236\n",
      "Epoch 30, loss: 2.115236\n",
      "Epoch 31, loss: 2.115236\n",
      "Epoch 32, loss: 2.115235\n",
      "Epoch 33, loss: 2.115235\n",
      "Epoch 34, loss: 2.115235\n",
      "Epoch 35, loss: 2.115234\n",
      "Epoch 36, loss: 2.115234\n",
      "Epoch 37, loss: 2.115234\n",
      "Epoch 38, loss: 2.115233\n",
      "Epoch 39, loss: 2.115233\n",
      "Epoch 40, loss: 2.115233\n",
      "Epoch 41, loss: 2.115233\n",
      "Epoch 42, loss: 2.115232\n",
      "Epoch 43, loss: 2.115232\n",
      "Epoch 44, loss: 2.115232\n",
      "Epoch 45, loss: 2.115231\n",
      "Epoch 46, loss: 2.115231\n",
      "Epoch 47, loss: 2.115231\n",
      "Epoch 48, loss: 2.115231\n",
      "Epoch 49, loss: 2.115230\n",
      "Epoch 50, loss: 2.115230\n",
      "Epoch 51, loss: 2.115230\n",
      "Epoch 52, loss: 2.115229\n",
      "Epoch 53, loss: 2.115229\n",
      "Epoch 54, loss: 2.115229\n",
      "Epoch 55, loss: 2.115228\n",
      "Epoch 56, loss: 2.115228\n",
      "Epoch 57, loss: 2.115228\n",
      "Epoch 58, loss: 2.115228\n",
      "Epoch 59, loss: 2.115227\n",
      "Epoch 60, loss: 2.115227\n",
      "Epoch 61, loss: 2.115227\n",
      "Epoch 62, loss: 2.115226\n",
      "Epoch 63, loss: 2.115226\n",
      "Epoch 64, loss: 2.115226\n",
      "Epoch 65, loss: 2.115226\n",
      "Epoch 66, loss: 2.115225\n",
      "Epoch 67, loss: 2.115225\n",
      "Epoch 68, loss: 2.115225\n",
      "Epoch 69, loss: 2.115224\n",
      "Epoch 70, loss: 2.115224\n",
      "Epoch 71, loss: 2.115224\n",
      "Epoch 72, loss: 2.115224\n",
      "Epoch 73, loss: 2.115223\n",
      "Epoch 74, loss: 2.115223\n",
      "Epoch 75, loss: 2.115223\n",
      "Epoch 76, loss: 2.115223\n",
      "Epoch 77, loss: 2.115222\n",
      "Epoch 78, loss: 2.115222\n",
      "Epoch 79, loss: 2.115222\n",
      "Epoch 80, loss: 2.115221\n",
      "Epoch 81, loss: 2.115221\n",
      "Epoch 82, loss: 2.115221\n",
      "Epoch 83, loss: 2.115220\n",
      "Epoch 84, loss: 2.115220\n",
      "Epoch 85, loss: 2.115220\n",
      "Epoch 86, loss: 2.115219\n",
      "Epoch 87, loss: 2.115219\n",
      "Epoch 88, loss: 2.115219\n",
      "Epoch 89, loss: 2.115218\n",
      "Epoch 90, loss: 2.115218\n",
      "Epoch 91, loss: 2.115218\n",
      "Epoch 92, loss: 2.115218\n",
      "Epoch 93, loss: 2.115218\n",
      "Epoch 94, loss: 2.115217\n",
      "Epoch 95, loss: 2.115217\n",
      "Epoch 96, loss: 2.115217\n",
      "Epoch 97, loss: 2.115216\n",
      "Epoch 98, loss: 2.115216\n",
      "Epoch 99, loss: 2.115216\n",
      "Epoch 100, loss: 2.115215\n",
      "Epoch 101, loss: 2.115215\n",
      "Epoch 102, loss: 2.115215\n",
      "Epoch 103, loss: 2.115214\n",
      "Epoch 104, loss: 2.115214\n",
      "Epoch 105, loss: 2.115214\n",
      "Epoch 106, loss: 2.115214\n",
      "Epoch 107, loss: 2.115213\n",
      "Epoch 108, loss: 2.115213\n",
      "Epoch 109, loss: 2.115213\n",
      "Epoch 110, loss: 2.115212\n",
      "Epoch 111, loss: 2.115212\n",
      "Epoch 112, loss: 2.115212\n",
      "Epoch 113, loss: 2.115212\n",
      "Epoch 114, loss: 2.115211\n",
      "Epoch 115, loss: 2.115211\n",
      "Epoch 116, loss: 2.115211\n",
      "Epoch 117, loss: 2.115210\n",
      "Epoch 118, loss: 2.115210\n",
      "Epoch 119, loss: 2.115210\n",
      "Epoch 120, loss: 2.115209\n",
      "Epoch 121, loss: 2.115209\n",
      "Epoch 122, loss: 2.115209\n",
      "Epoch 123, loss: 2.115209\n",
      "Epoch 124, loss: 2.115208\n",
      "Epoch 125, loss: 2.115208\n",
      "Epoch 126, loss: 2.115208\n",
      "Epoch 127, loss: 2.115208\n",
      "Epoch 128, loss: 2.115207\n",
      "Epoch 129, loss: 2.115207\n",
      "Epoch 130, loss: 2.115207\n",
      "Epoch 131, loss: 2.115206\n",
      "Epoch 132, loss: 2.115206\n",
      "Epoch 133, loss: 2.115206\n",
      "Epoch 134, loss: 2.115205\n",
      "Epoch 135, loss: 2.115205\n",
      "Epoch 136, loss: 2.115205\n",
      "Epoch 137, loss: 2.115205\n",
      "Epoch 138, loss: 2.115204\n",
      "Epoch 139, loss: 2.115204\n",
      "Epoch 140, loss: 2.115204\n",
      "Epoch 141, loss: 2.115203\n",
      "Epoch 142, loss: 2.115203\n",
      "Epoch 143, loss: 2.115203\n",
      "Epoch 144, loss: 2.115202\n",
      "Epoch 145, loss: 2.115202\n",
      "Epoch 146, loss: 2.115202\n",
      "Epoch 147, loss: 2.115202\n",
      "Epoch 148, loss: 2.115201\n",
      "Epoch 149, loss: 2.115201\n",
      "Epoch 150, loss: 2.115201\n",
      "Epoch 151, loss: 2.115200\n",
      "Epoch 152, loss: 2.115200\n",
      "Epoch 153, loss: 2.115200\n",
      "Epoch 154, loss: 2.115199\n",
      "Epoch 155, loss: 2.115199\n",
      "Epoch 156, loss: 2.115199\n",
      "Epoch 157, loss: 2.115199\n",
      "Epoch 158, loss: 2.115198\n",
      "Epoch 159, loss: 2.115198\n",
      "Epoch 160, loss: 2.115198\n",
      "Epoch 161, loss: 2.115198\n",
      "Epoch 162, loss: 2.115197\n",
      "Epoch 163, loss: 2.115197\n",
      "Epoch 164, loss: 2.115197\n",
      "Epoch 165, loss: 2.115196\n",
      "Epoch 166, loss: 2.115196\n",
      "Epoch 167, loss: 2.115196\n",
      "Epoch 168, loss: 2.115195\n",
      "Epoch 169, loss: 2.115195\n",
      "Epoch 170, loss: 2.115195\n",
      "Epoch 171, loss: 2.115195\n",
      "Epoch 172, loss: 2.115194\n",
      "Epoch 173, loss: 2.115194\n",
      "Epoch 174, loss: 2.115194\n",
      "Epoch 175, loss: 2.115193\n",
      "Epoch 176, loss: 2.115193\n",
      "Epoch 177, loss: 2.115193\n",
      "Epoch 178, loss: 2.115193\n",
      "Epoch 179, loss: 2.115192\n",
      "Epoch 180, loss: 2.115192\n",
      "Epoch 181, loss: 2.115192\n",
      "Epoch 182, loss: 2.115191\n",
      "Epoch 183, loss: 2.115191\n",
      "Epoch 184, loss: 2.115191\n",
      "Epoch 185, loss: 2.115191\n",
      "Epoch 186, loss: 2.115190\n",
      "Epoch 187, loss: 2.115190\n",
      "Epoch 188, loss: 2.115190\n",
      "Epoch 189, loss: 2.115189\n",
      "Epoch 190, loss: 2.115189\n",
      "Epoch 191, loss: 2.115189\n",
      "Epoch 192, loss: 2.115188\n",
      "Epoch 193, loss: 2.115188\n",
      "Epoch 194, loss: 2.115188\n",
      "Epoch 195, loss: 2.115188\n",
      "Epoch 196, loss: 2.115187\n",
      "Epoch 197, loss: 2.115187\n",
      "Epoch 198, loss: 2.115187\n",
      "Epoch 199, loss: 2.115186\n",
      "Epoch 0, loss: 2.114746\n",
      "Epoch 1, loss: 2.114746\n",
      "Epoch 2, loss: 2.114745\n",
      "Epoch 3, loss: 2.114745\n",
      "Epoch 4, loss: 2.114745\n",
      "Epoch 5, loss: 2.114744\n",
      "Epoch 6, loss: 2.114744\n",
      "Epoch 7, loss: 2.114744\n",
      "Epoch 8, loss: 2.114744\n",
      "Epoch 9, loss: 2.114743\n",
      "Epoch 10, loss: 2.114743\n",
      "Epoch 11, loss: 2.114743\n",
      "Epoch 12, loss: 2.114742\n",
      "Epoch 13, loss: 2.114742\n",
      "Epoch 14, loss: 2.114742\n",
      "Epoch 15, loss: 2.114741\n",
      "Epoch 16, loss: 2.114741\n",
      "Epoch 17, loss: 2.114741\n",
      "Epoch 18, loss: 2.114741\n",
      "Epoch 19, loss: 2.114740\n",
      "Epoch 20, loss: 2.114740\n",
      "Epoch 21, loss: 2.114740\n",
      "Epoch 22, loss: 2.114739\n",
      "Epoch 23, loss: 2.114739\n",
      "Epoch 24, loss: 2.114739\n",
      "Epoch 25, loss: 2.114738\n",
      "Epoch 26, loss: 2.114738\n",
      "Epoch 27, loss: 2.114738\n",
      "Epoch 28, loss: 2.114737\n",
      "Epoch 29, loss: 2.114737\n",
      "Epoch 30, loss: 2.114737\n",
      "Epoch 31, loss: 2.114737\n",
      "Epoch 32, loss: 2.114736\n",
      "Epoch 33, loss: 2.114736\n",
      "Epoch 34, loss: 2.114736\n",
      "Epoch 35, loss: 2.114735\n",
      "Epoch 36, loss: 2.114735\n",
      "Epoch 37, loss: 2.114735\n",
      "Epoch 38, loss: 2.114735\n",
      "Epoch 39, loss: 2.114734\n",
      "Epoch 40, loss: 2.114734\n",
      "Epoch 41, loss: 2.114734\n",
      "Epoch 42, loss: 2.114733\n",
      "Epoch 43, loss: 2.114733\n",
      "Epoch 44, loss: 2.114733\n",
      "Epoch 45, loss: 2.114732\n",
      "Epoch 46, loss: 2.114732\n",
      "Epoch 47, loss: 2.114732\n",
      "Epoch 48, loss: 2.114732\n",
      "Epoch 49, loss: 2.114731\n",
      "Epoch 50, loss: 2.114731\n",
      "Epoch 51, loss: 2.114731\n",
      "Epoch 52, loss: 2.114730\n",
      "Epoch 53, loss: 2.114730\n",
      "Epoch 54, loss: 2.114730\n",
      "Epoch 55, loss: 2.114730\n",
      "Epoch 56, loss: 2.114729\n",
      "Epoch 57, loss: 2.114729\n",
      "Epoch 58, loss: 2.114729\n",
      "Epoch 59, loss: 2.114728\n",
      "Epoch 60, loss: 2.114728\n",
      "Epoch 61, loss: 2.114728\n",
      "Epoch 62, loss: 2.114727\n",
      "Epoch 63, loss: 2.114727\n",
      "Epoch 64, loss: 2.114727\n",
      "Epoch 65, loss: 2.114726\n",
      "Epoch 66, loss: 2.114726\n",
      "Epoch 67, loss: 2.114726\n",
      "Epoch 68, loss: 2.114726\n",
      "Epoch 69, loss: 2.114725\n",
      "Epoch 70, loss: 2.114725\n",
      "Epoch 71, loss: 2.114725\n",
      "Epoch 72, loss: 2.114724\n",
      "Epoch 73, loss: 2.114724\n",
      "Epoch 74, loss: 2.114724\n",
      "Epoch 75, loss: 2.114724\n",
      "Epoch 76, loss: 2.114723\n",
      "Epoch 77, loss: 2.114723\n",
      "Epoch 78, loss: 2.114723\n",
      "Epoch 79, loss: 2.114722\n",
      "Epoch 80, loss: 2.114722\n",
      "Epoch 81, loss: 2.114722\n",
      "Epoch 82, loss: 2.114722\n",
      "Epoch 83, loss: 2.114721\n",
      "Epoch 84, loss: 2.114721\n",
      "Epoch 85, loss: 2.114721\n",
      "Epoch 86, loss: 2.114720\n",
      "Epoch 87, loss: 2.114720\n",
      "Epoch 88, loss: 2.114720\n",
      "Epoch 89, loss: 2.114719\n",
      "Epoch 90, loss: 2.114719\n",
      "Epoch 91, loss: 2.114719\n",
      "Epoch 92, loss: 2.114719\n",
      "Epoch 93, loss: 2.114718\n",
      "Epoch 94, loss: 2.114718\n",
      "Epoch 95, loss: 2.114718\n",
      "Epoch 96, loss: 2.114717\n",
      "Epoch 97, loss: 2.114717\n",
      "Epoch 98, loss: 2.114717\n",
      "Epoch 99, loss: 2.114716\n",
      "Epoch 100, loss: 2.114716\n",
      "Epoch 101, loss: 2.114716\n",
      "Epoch 102, loss: 2.114715\n",
      "Epoch 103, loss: 2.114715\n",
      "Epoch 104, loss: 2.114715\n",
      "Epoch 105, loss: 2.114715\n",
      "Epoch 106, loss: 2.114714\n",
      "Epoch 107, loss: 2.114714\n",
      "Epoch 108, loss: 2.114714\n",
      "Epoch 109, loss: 2.114713\n",
      "Epoch 110, loss: 2.114713\n",
      "Epoch 111, loss: 2.114713\n",
      "Epoch 112, loss: 2.114713\n",
      "Epoch 113, loss: 2.114712\n",
      "Epoch 114, loss: 2.114712\n",
      "Epoch 115, loss: 2.114712\n",
      "Epoch 116, loss: 2.114711\n",
      "Epoch 117, loss: 2.114711\n",
      "Epoch 118, loss: 2.114711\n",
      "Epoch 119, loss: 2.114710\n",
      "Epoch 120, loss: 2.114710\n",
      "Epoch 121, loss: 2.114710\n",
      "Epoch 122, loss: 2.114710\n",
      "Epoch 123, loss: 2.114709\n",
      "Epoch 124, loss: 2.114709\n",
      "Epoch 125, loss: 2.114708\n",
      "Epoch 126, loss: 2.114708\n",
      "Epoch 127, loss: 2.114708\n",
      "Epoch 128, loss: 2.114708\n",
      "Epoch 129, loss: 2.114707\n",
      "Epoch 130, loss: 2.114707\n",
      "Epoch 131, loss: 2.114707\n",
      "Epoch 132, loss: 2.114707\n",
      "Epoch 133, loss: 2.114706\n",
      "Epoch 134, loss: 2.114706\n",
      "Epoch 135, loss: 2.114706\n",
      "Epoch 136, loss: 2.114705\n",
      "Epoch 137, loss: 2.114705\n",
      "Epoch 138, loss: 2.114705\n",
      "Epoch 139, loss: 2.114704\n",
      "Epoch 140, loss: 2.114704\n",
      "Epoch 141, loss: 2.114704\n",
      "Epoch 142, loss: 2.114704\n",
      "Epoch 143, loss: 2.114703\n",
      "Epoch 144, loss: 2.114703\n",
      "Epoch 145, loss: 2.114702\n",
      "Epoch 146, loss: 2.114702\n",
      "Epoch 147, loss: 2.114702\n",
      "Epoch 148, loss: 2.114702\n",
      "Epoch 149, loss: 2.114702\n",
      "Epoch 150, loss: 2.114701\n",
      "Epoch 151, loss: 2.114701\n",
      "Epoch 152, loss: 2.114701\n",
      "Epoch 153, loss: 2.114700\n",
      "Epoch 154, loss: 2.114700\n",
      "Epoch 155, loss: 2.114700\n",
      "Epoch 156, loss: 2.114699\n",
      "Epoch 157, loss: 2.114699\n",
      "Epoch 158, loss: 2.114699\n",
      "Epoch 159, loss: 2.114699\n",
      "Epoch 160, loss: 2.114698\n",
      "Epoch 161, loss: 2.114698\n",
      "Epoch 162, loss: 2.114697\n",
      "Epoch 163, loss: 2.114697\n",
      "Epoch 164, loss: 2.114697\n",
      "Epoch 165, loss: 2.114697\n",
      "Epoch 166, loss: 2.114696\n",
      "Epoch 167, loss: 2.114696\n",
      "Epoch 168, loss: 2.114696\n",
      "Epoch 169, loss: 2.114696\n",
      "Epoch 170, loss: 2.114695\n",
      "Epoch 171, loss: 2.114695\n",
      "Epoch 172, loss: 2.114694\n",
      "Epoch 173, loss: 2.114694\n",
      "Epoch 174, loss: 2.114694\n",
      "Epoch 175, loss: 2.114694\n",
      "Epoch 176, loss: 2.114693\n",
      "Epoch 177, loss: 2.114693\n",
      "Epoch 178, loss: 2.114693\n",
      "Epoch 179, loss: 2.114693\n",
      "Epoch 180, loss: 2.114692\n",
      "Epoch 181, loss: 2.114692\n",
      "Epoch 182, loss: 2.114691\n",
      "Epoch 183, loss: 2.114691\n",
      "Epoch 184, loss: 2.114691\n",
      "Epoch 185, loss: 2.114691\n",
      "Epoch 186, loss: 2.114691\n",
      "Epoch 187, loss: 2.114690\n",
      "Epoch 188, loss: 2.114690\n",
      "Epoch 189, loss: 2.114690\n",
      "Epoch 190, loss: 2.114689\n",
      "Epoch 191, loss: 2.114689\n",
      "Epoch 192, loss: 2.114689\n",
      "Epoch 193, loss: 2.114688\n",
      "Epoch 194, loss: 2.114688\n",
      "Epoch 195, loss: 2.114688\n",
      "Epoch 196, loss: 2.114687\n",
      "Epoch 197, loss: 2.114687\n",
      "Epoch 198, loss: 2.114687\n",
      "Epoch 199, loss: 2.114687\n",
      "Epoch 0, loss: 2.114642\n",
      "Epoch 1, loss: 2.114642\n",
      "Epoch 2, loss: 2.114642\n",
      "Epoch 3, loss: 2.114641\n",
      "Epoch 4, loss: 2.114641\n",
      "Epoch 5, loss: 2.114641\n",
      "Epoch 6, loss: 2.114640\n",
      "Epoch 7, loss: 2.114640\n",
      "Epoch 8, loss: 2.114640\n",
      "Epoch 9, loss: 2.114639\n",
      "Epoch 10, loss: 2.114639\n",
      "Epoch 11, loss: 2.114639\n",
      "Epoch 12, loss: 2.114638\n",
      "Epoch 13, loss: 2.114638\n",
      "Epoch 14, loss: 2.114638\n",
      "Epoch 15, loss: 2.114638\n",
      "Epoch 16, loss: 2.114637\n",
      "Epoch 17, loss: 2.114637\n",
      "Epoch 18, loss: 2.114637\n",
      "Epoch 19, loss: 2.114636\n",
      "Epoch 20, loss: 2.114636\n",
      "Epoch 21, loss: 2.114636\n",
      "Epoch 22, loss: 2.114636\n",
      "Epoch 23, loss: 2.114635\n",
      "Epoch 24, loss: 2.114635\n",
      "Epoch 25, loss: 2.114635\n",
      "Epoch 26, loss: 2.114634\n",
      "Epoch 27, loss: 2.114634\n",
      "Epoch 28, loss: 2.114634\n",
      "Epoch 29, loss: 2.114634\n",
      "Epoch 30, loss: 2.114633\n",
      "Epoch 31, loss: 2.114633\n",
      "Epoch 32, loss: 2.114633\n",
      "Epoch 33, loss: 2.114632\n",
      "Epoch 34, loss: 2.114632\n",
      "Epoch 35, loss: 2.114632\n",
      "Epoch 36, loss: 2.114631\n",
      "Epoch 37, loss: 2.114631\n",
      "Epoch 38, loss: 2.114631\n",
      "Epoch 39, loss: 2.114631\n",
      "Epoch 40, loss: 2.114630\n",
      "Epoch 41, loss: 2.114630\n",
      "Epoch 42, loss: 2.114630\n",
      "Epoch 43, loss: 2.114629\n",
      "Epoch 44, loss: 2.114629\n",
      "Epoch 45, loss: 2.114629\n",
      "Epoch 46, loss: 2.114628\n",
      "Epoch 47, loss: 2.114628\n",
      "Epoch 48, loss: 2.114628\n",
      "Epoch 49, loss: 2.114627\n",
      "Epoch 50, loss: 2.114627\n",
      "Epoch 51, loss: 2.114627\n",
      "Epoch 52, loss: 2.114627\n",
      "Epoch 53, loss: 2.114626\n",
      "Epoch 54, loss: 2.114626\n",
      "Epoch 55, loss: 2.114626\n",
      "Epoch 56, loss: 2.114625\n",
      "Epoch 57, loss: 2.114625\n",
      "Epoch 58, loss: 2.114625\n",
      "Epoch 59, loss: 2.114624\n",
      "Epoch 60, loss: 2.114624\n",
      "Epoch 61, loss: 2.114624\n",
      "Epoch 62, loss: 2.114624\n",
      "Epoch 63, loss: 2.114623\n",
      "Epoch 64, loss: 2.114623\n",
      "Epoch 65, loss: 2.114623\n",
      "Epoch 66, loss: 2.114623\n",
      "Epoch 67, loss: 2.114622\n",
      "Epoch 68, loss: 2.114622\n",
      "Epoch 69, loss: 2.114621\n",
      "Epoch 70, loss: 2.114621\n",
      "Epoch 71, loss: 2.114621\n",
      "Epoch 72, loss: 2.114621\n",
      "Epoch 73, loss: 2.114620\n",
      "Epoch 74, loss: 2.114620\n",
      "Epoch 75, loss: 2.114620\n",
      "Epoch 76, loss: 2.114619\n",
      "Epoch 77, loss: 2.114619\n",
      "Epoch 78, loss: 2.114619\n",
      "Epoch 79, loss: 2.114619\n",
      "Epoch 80, loss: 2.114618\n",
      "Epoch 81, loss: 2.114618\n",
      "Epoch 82, loss: 2.114618\n",
      "Epoch 83, loss: 2.114618\n",
      "Epoch 84, loss: 2.114617\n",
      "Epoch 85, loss: 2.114617\n",
      "Epoch 86, loss: 2.114616\n",
      "Epoch 87, loss: 2.114616\n",
      "Epoch 88, loss: 2.114616\n",
      "Epoch 89, loss: 2.114616\n",
      "Epoch 90, loss: 2.114615\n",
      "Epoch 91, loss: 2.114615\n",
      "Epoch 92, loss: 2.114615\n",
      "Epoch 93, loss: 2.114614\n",
      "Epoch 94, loss: 2.114614\n",
      "Epoch 95, loss: 2.114614\n",
      "Epoch 96, loss: 2.114614\n",
      "Epoch 97, loss: 2.114613\n",
      "Epoch 98, loss: 2.114613\n",
      "Epoch 99, loss: 2.114612\n",
      "Epoch 100, loss: 2.114612\n",
      "Epoch 101, loss: 2.114612\n",
      "Epoch 102, loss: 2.114612\n",
      "Epoch 103, loss: 2.114611\n",
      "Epoch 104, loss: 2.114611\n",
      "Epoch 105, loss: 2.114611\n",
      "Epoch 106, loss: 2.114610\n",
      "Epoch 107, loss: 2.114610\n",
      "Epoch 108, loss: 2.114610\n",
      "Epoch 109, loss: 2.114610\n",
      "Epoch 110, loss: 2.114609\n",
      "Epoch 111, loss: 2.114609\n",
      "Epoch 112, loss: 2.114609\n",
      "Epoch 113, loss: 2.114609\n",
      "Epoch 114, loss: 2.114608\n",
      "Epoch 115, loss: 2.114608\n",
      "Epoch 116, loss: 2.114607\n",
      "Epoch 117, loss: 2.114607\n",
      "Epoch 118, loss: 2.114607\n",
      "Epoch 119, loss: 2.114606\n",
      "Epoch 120, loss: 2.114606\n",
      "Epoch 121, loss: 2.114606\n",
      "Epoch 122, loss: 2.114606\n",
      "Epoch 123, loss: 2.114606\n",
      "Epoch 124, loss: 2.114605\n",
      "Epoch 125, loss: 2.114605\n",
      "Epoch 126, loss: 2.114605\n",
      "Epoch 127, loss: 2.114604\n",
      "Epoch 128, loss: 2.114604\n",
      "Epoch 129, loss: 2.114604\n",
      "Epoch 130, loss: 2.114603\n",
      "Epoch 131, loss: 2.114603\n",
      "Epoch 132, loss: 2.114603\n",
      "Epoch 133, loss: 2.114602\n",
      "Epoch 134, loss: 2.114602\n",
      "Epoch 135, loss: 2.114602\n",
      "Epoch 136, loss: 2.114601\n",
      "Epoch 137, loss: 2.114601\n",
      "Epoch 138, loss: 2.114601\n",
      "Epoch 139, loss: 2.114601\n",
      "Epoch 140, loss: 2.114600\n",
      "Epoch 141, loss: 2.114600\n",
      "Epoch 142, loss: 2.114600\n",
      "Epoch 143, loss: 2.114600\n",
      "Epoch 144, loss: 2.114599\n",
      "Epoch 145, loss: 2.114599\n",
      "Epoch 146, loss: 2.114598\n",
      "Epoch 147, loss: 2.114598\n",
      "Epoch 148, loss: 2.114598\n",
      "Epoch 149, loss: 2.114598\n",
      "Epoch 150, loss: 2.114597\n",
      "Epoch 151, loss: 2.114597\n",
      "Epoch 152, loss: 2.114597\n",
      "Epoch 153, loss: 2.114597\n",
      "Epoch 154, loss: 2.114596\n",
      "Epoch 155, loss: 2.114596\n",
      "Epoch 156, loss: 2.114596\n",
      "Epoch 157, loss: 2.114595\n",
      "Epoch 158, loss: 2.114595\n",
      "Epoch 159, loss: 2.114595\n",
      "Epoch 160, loss: 2.114594\n",
      "Epoch 161, loss: 2.114594\n",
      "Epoch 162, loss: 2.114594\n",
      "Epoch 163, loss: 2.114593\n",
      "Epoch 164, loss: 2.114593\n",
      "Epoch 165, loss: 2.114593\n",
      "Epoch 166, loss: 2.114593\n",
      "Epoch 167, loss: 2.114592\n",
      "Epoch 168, loss: 2.114592\n",
      "Epoch 169, loss: 2.114592\n",
      "Epoch 170, loss: 2.114592\n",
      "Epoch 171, loss: 2.114591\n",
      "Epoch 172, loss: 2.114591\n",
      "Epoch 173, loss: 2.114590\n",
      "Epoch 174, loss: 2.114590\n",
      "Epoch 175, loss: 2.114590\n",
      "Epoch 176, loss: 2.114590\n",
      "Epoch 177, loss: 2.114589\n",
      "Epoch 178, loss: 2.114589\n",
      "Epoch 179, loss: 2.114589\n",
      "Epoch 180, loss: 2.114588\n",
      "Epoch 181, loss: 2.114588\n",
      "Epoch 182, loss: 2.114588\n",
      "Epoch 183, loss: 2.114587\n",
      "Epoch 184, loss: 2.114587\n",
      "Epoch 185, loss: 2.114587\n",
      "Epoch 186, loss: 2.114587\n",
      "Epoch 187, loss: 2.114586\n",
      "Epoch 188, loss: 2.114586\n",
      "Epoch 189, loss: 2.114586\n",
      "Epoch 190, loss: 2.114586\n",
      "Epoch 191, loss: 2.114585\n",
      "Epoch 192, loss: 2.114585\n",
      "Epoch 193, loss: 2.114584\n",
      "Epoch 194, loss: 2.114584\n",
      "Epoch 195, loss: 2.114584\n",
      "Epoch 196, loss: 2.114584\n",
      "Epoch 197, loss: 2.114583\n",
      "Epoch 198, loss: 2.114583\n",
      "Epoch 199, loss: 2.114583\n",
      "best validation accuracy achieved: 0.249000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in reg_strengths:\n",
    "        loss_history = classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=i, batch_size=batch_size, reg=j)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy is None or best_val_accuracy < accuracy:\n",
    "            best_classifier = classifier\n",
    "            best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.202000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
